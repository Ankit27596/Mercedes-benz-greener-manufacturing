{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_pipeline1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "from numpy import loadtxt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4HX4CBScmcVe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stacked import StackingAveragedModels"
      ],
      "metadata": {
        "id": "uKhLnbGG-WOe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  data['X118_add_X263']=data.apply(lambda row: row.X118 + row.X263, axis=1)\n",
        "  data['X136_add_X261']=data.apply(lambda row: row.X136 + row.X261, axis=1)\n",
        "  data['X275_add_X314']=data.apply(lambda row: row.X275 + row.X314, axis=1)\n",
        "  data['X118_add_X263_add_X136']=data.apply(lambda row: row.X118 + row.X263 + row.X136, axis=1)\n",
        "  data['X314_add_X261_add_X275']=data.apply(lambda row: row.X314 + row.X261 + row.X275, axis=1)\n",
        "  data['X276_add_X118_add_X261']=data.apply(lambda row: row.X276 + row.X118 + row.X261, axis=1)\n",
        "  \n",
        "  with open('y_train.pkl', 'rb') as a:\n",
        "    y = pickle.load(a)\n",
        "  y_mean=y.mean()\n",
        "  with open('trainX0.pkl','rb') as b:\n",
        "    trainX0=pickle.load(b)\n",
        "  with open('trainX1.pkl','rb') as c:\n",
        "    trainX1=pickle.load(c)\n",
        "  with open('trainX2.pkl','rb') as d:\n",
        "    trainX2=pickle.load(d)\n",
        "  with open('trainX3.pkl','rb') as e:\n",
        "    trainX3=pickle.load(e)\n",
        "  with open('trainX4.pkl','rb') as g:\n",
        "    trainX4=pickle.load(g)\n",
        "  with open('trainX5.pkl','rb') as h:\n",
        "    trainX5=pickle.load(h)\n",
        "  with open('trainX6.pkl','rb') as i:\n",
        "    trainX6=pickle.load(i)\n",
        "  with open('trainX8.pkl','rb') as j:\n",
        "    trainX8=pickle.load(j)\n",
        "  data[\"X0\"]=[trainX0.loc[X[\"X0\"].iloc[0]] if data[\"X0\"].iloc[0] in trainX0.index else y_mean ]\n",
        "  data[\"X1\"]=[trainX1.loc[X[\"X1\"].iloc[0]] if data[\"X1\"].iloc[0] in trainX1.index else y_mean ]\n",
        "  data[\"X2\"]=[trainX2.loc[X[\"X2\"].iloc[0]] if data[\"X2\"].iloc[0] in trainX2.index else y_mean ]\n",
        "  data[\"X3\"]=[trainX3.loc[X[\"X3\"].iloc[0]] if data[\"X3\"].iloc[0] in trainX3.index else y_mean ]\n",
        "  data[\"X4\"]=[trainX4.loc[X[\"X4\"].iloc[0]] if data[\"X4\"].iloc[0] in trainX4.index else y_mean ]\n",
        "  data[\"X5\"]=[trainX5.loc[X[\"X5\"].iloc[0]] if data[\"X5\"].iloc[0] in trainX5.index else y_mean ]\n",
        "  data[\"X6\"]=[trainX6.loc[X[\"X6\"].iloc[0]] if data[\"X6\"].iloc[0] in trainX6.index else y_mean ]\n",
        "  data[\"X8\"]=[trainX8.loc[X[\"X8\"].iloc[0]] if data[\"X8\"].iloc[0] in trainX8.index else y_mean ]\n",
        "  with open('drop_ft.txt', 'r') as f:\n",
        "    drop_feat = json.loads(f.read())\n",
        "  for i in drop_feat:\n",
        "    if i in data.columns:\n",
        "      data.drop(i, axis=1, inplace=True)\n",
        "  x_train_mean=pd.read_csv('x_train_mean.csv')\n",
        "  std=StandardScaler()\n",
        "  data_std=std.fit_transform(x_train_mean)\n",
        "  dt=std.transform(data)\n",
        "  #tr=pd.read_csv('train_df.csv')\n",
        "  pca=PCA(n_components=90)\n",
        "  pca_train_mean=pca.fit_transform(data_std)\n",
        "  pca_test_mean=pca.transform(dt)\n",
        "  return pca_test_mean\n",
        "  "
      ],
      "metadata": {
        "id": "wqY_2xtLW8k4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stacked_averaged_models.pkl', 'rb') as f:\n",
        "  users = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4d0AvnQcFvI",
        "outputId": "32386251-dc0c-494e-d7bb-d022a70db0fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(x):\n",
        "  a=preprocess(x)\n",
        "  predict_value=users.predict(a)\n",
        "  print('The predicted time is:{} seconds'.format(predict_value))"
      ],
      "metadata": {
        "id": "b0Bx4RlIwrpX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test=pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "B8C_x6SjoItJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "rand=random.choice(test.values)\n",
        "query_point=pd.DataFrame(data=rand.reshape(1,-1),columns=test.columns)"
      ],
      "metadata": {
        "id": "jA9VvXqvn58V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(query_point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3nkBS-5No8P",
        "outputId": "1cfba93b-22d4-43b5-c26c-42d88acc906c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted time is:[93.15795264] seconds\n"
          ]
        }
      ]
    }
  ]
}